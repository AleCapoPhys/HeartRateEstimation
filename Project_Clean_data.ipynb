{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEART RATE ESTIMATION\n",
    "\n",
    "Seismocardiography([SCG](https://www.ncbi.nlm.nih.gov/pubmed/24111357)) is a very promising technique to measure Heart Rate (HR) and Respiratory Rate (RR) with the detector positioned above sternum. It is generally based on accelerometer and gyroscope readings or a combination of them. \n",
    "\n",
    "Ballistocardiography([BCG](https://en.wikipedia.org/wiki/Ballistocardiography)) is an another technique to estimate heart and respiratory rate with combination of both accelerometer and gyroscope. It is an indirect evaluation of HR and RR since the contact between the device and the body of the subject is not required (e.g., accelerometer platform mounted under the slats of the bed).   \n",
    "  \n",
    "MuSe(Multi-Sensor miniaturized, low-power, wireless [IMU](https://en.wikipedia.org/wiki/Inertial_measurement_unit)) is an Inertial Measurement Unit (IMU) provide by [221e](https://www.221e.com). In the context of this project, It allows to record the inertial data necessary for the estimation of SCG and BCG.\n",
    "\n",
    "The goal of this assignment is to estimate the heart rate of an healthy subject, given linear acceleration and angular velocity measurements recorded by using the aforementioned MuSe platform. \n",
    "The study must be performed on two datasets: the first is the compulsory one (**center_sternum.txt**) while the second is left at the discretion of the group, among those made available for the assignment.\n",
    "\n",
    "**N.B: Remember that normal beat is around [40-100] bpm.**\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The data is provided in .txt file. During this study two healthy subjects were involved with their informed consent. The first dataset was recorded on one subject, while all the other datasets were recorded on the second subject. \n",
    "\n",
    "This is the first mandatory file:\n",
    "\n",
    "* **center_sternum.txt**: MuSe placed on the center of the sternum. The subject was lying supine on his left and right side, respectively. \n",
    "\n",
    "\n",
    "Choose one of the following files in order to complete the task.\n",
    "\n",
    "1. **1_Stave_supine_static.txt**: Sensor placed on a bed stave, under the mattress at the level of the chest. The subject was lying supine on his left and right side. \n",
    "* **2_Mattress_supine.txt**: Sensor placed on the mattress, near one corner but not under the pillow. The subject laid in the same position as above. \n",
    "* **3_Subject_sitting_chair.txt**: Sensor placed on the desk: the subject, sitting on a chair, leaned forearms and hands on the desk. \n",
    "* **4_Chest_sweater.txt**: Sensor placed on the subject chest directly on a sweater. \n",
    "* **5_Under_chair.txt**: Subject sitting on a chair, sensor placed under the seat of the chair. \n",
    "\n",
    "All .txt files give 16 columns index, in particular: \n",
    "\n",
    "* Log Freq stands for the acquisition  in Hz (i.e., sampling interval is constant).\n",
    "* AccX, AccY, AccZ are the measured magnitude of linear acceleration along each axis.\n",
    "* GyroX, GyroY, GyroZ are the measured magnitude of angular velocity along each axis.\n",
    "* MagnX, MagnY, MagnZ are the measured magnitude of magnetic field along each axis.\n",
    "* qw, qi, qj, qk are the quaternion components, representing the spatial orientation of the Muse system.\n",
    "\n",
    "Each dataset includes, in addition to the data, one file containing the adopted configuration of the MuSe(**README1.txt** for the first measurement, and in **README_5.txt** for the other measurement).\n",
    " \n",
    "\n",
    "\n",
    "## Assignment\n",
    "\n",
    "\n",
    "\n",
    "1. Data preparation:\n",
    "\n",
    "    1.1. Load the txt file and select only the columns you are interesting in, in order to do a complete data analysis (e.g. Log Freq, AccX, ... )\n",
    "    \n",
    "    1.2. Plot selected data in function of time and choose a properly time window over which to perform the analysis. Pay attention on time rappresentation and the measurament unit.\n",
    "    \n",
    "    1.3. In order to make an appropiate work, decide if take care about some particular axis or some combination of them as well as derived features for the next step of the task. Motivate your choice.  \n",
    "\n",
    "\n",
    "    \n",
    "2. Time and frequency analysis:\n",
    "\n",
    "    2.1. Statistical analysis: provide a statistical description of the chosen dataset. Statistical descriptors includes for example mean, median, variance, standard deviation, 25th and 75th percentiles, and correlation coefficients. Investigate what could be the most interesting descriptors for this type of data, motivating the choices.\n",
    "    \n",
    "    2.2. Fourier Analysis: Perform a frequency analysis of the data. Look at the spectrum and explain what you see. Use this step in order to properly design the filters in the following step. \n",
    "\n",
    "\n",
    "\n",
    "3. Filter:\n",
    "    \n",
    "    Implement your own filter, trying to extrapolate heart rate signal. Hint:\n",
    "    \n",
    "    (a) Directly from Fourier Analysis, antitrasform data looking for the most interesting frequency band.\n",
    "    \n",
    "    (b) Choose the appropriate Lowpass/Bandpass/Highpass filter.\n",
    "    \n",
    "    (c) Wavelet trasform (a powerfull instrument that make a time and frequency analysis of signal).\n",
    "    \n",
    "    (d) Find another method by yourselves.\n",
    "    \n",
    "    Motivate your choice.\n",
    "    \n",
    "    \n",
    "4. Metrics:\n",
    "\n",
    "    4.1. Heart Beat Per Minute(BPM): extrapolate BPM, make an histogram of the result. Does it follow a partiular distribution? \n",
    "    \n",
    "    4.2. Heart Rate Variability(HRV): extrapolate HRV, explain why this parameter is important, and plot the results.\n",
    "\n",
    "\n",
    "\n",
    "5. (OPTIONAL) Algorithm: Elaborate a simple algorithm to extrapolate heart beat even when filter failed (e.g. look at particular threshold...).  \n",
    "\n",
    "\n",
    "\n",
    "6. Conclusion: \n",
    "\n",
    "    Summarise the obtained results, in particular making a comparison between the two files analysed. Highlight limitation and critical issues encountered during the work, motivating the most relevant contribution given by your solution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**N.B: Indicate the contribution, to achieving the result, of each member of the group.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEART RATE ESTIMATION PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "#### Understanding and selection of the data\n",
    "\n",
    "First of all, we need to understand the data we are working with and prepare it for our\n",
    "task. The aim of this project is to compare and analyze Heart Rate of healthy subjects measured with\n",
    "two different devices. Therefore, we look at the available data and select the adequate\n",
    "variables to study heart rate and we prepare them for our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variables in our datafile are: Index(['Log Mode', 'Log Freq', 'Timestamp', 'AccX', 'AccY', 'AccZ', 'GyroX',\n",
      "       'GyroY', 'GyroZ', 'MagnX', 'MagnY', 'MagnZ', 'qw', 'qi', 'qj', 'qk'],\n",
      "      dtype='object')\n",
      "The size of our datafile is (16506, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load the datafile\n",
    "datafile = pd.read_csv('center_sternum.txt',sep='\\t')\n",
    "\n",
    "# Look what our data is like in order to select useful columns\n",
    "print(\"The variables in our datafile are:\", datafile.columns)\n",
    "print(\"The size of our datafile is\", datafile.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop the unimportant columns, and we correct the data with the manufacturer's calibration file. Note that in the calibration matrix off-diagonal terms are very small, but non-zero. In this cell there is also the calibration for the second dataset, this is because the code we have written is very versatile: by just changing the imported filename and the calibration, the whole analysis is carried out by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not needed columns\n",
    "df = datafile.drop(['qw', 'qi', 'qj', 'qk'], axis = 1)\n",
    "\n",
    "# Calibrate the data considering the used device\n",
    "# First device cal data\n",
    "\n",
    "gyro_offset = np.array([-2.242224,2.963463,-0.718397])\n",
    "\n",
    "acc_calibration = np.array([[1.000966,-0.002326418,-0.0006995499],\n",
    "                    [-0.002326379,0.9787045,-0.001540918],\n",
    "                    [-0.0006995811,-0.001540928,1.00403]])\n",
    "acc_offset = np.array([-3.929942,-13.74679,60.67546])\n",
    "\n",
    "magn_calibration = np.array([[0.9192851,-0.02325168,0.003480837],\n",
    "                   [-0.02325175,0.914876,0.004257396],\n",
    "                   [0.003481006,0.004257583,0.8748001]])\n",
    "magn_offset = np.array([-95.67974,-244.9142,17.71132])\n",
    "'''\n",
    "#Second device cal data\n",
    "gyro_offset = np.array([-2.804399,1.793105,0.3411708])\n",
    "\n",
    "acc_calibration = np.array([[1.002982,9.415505E-05,0.004346743],\n",
    "                    [9.04459E-05, 1.002731,-0.001444198],\n",
    "                    [0.004346536,-0.001444751,1.030587]])\n",
    "acc_offset = np.array([3.602701,-20.96658,54.97186])\n",
    "\n",
    "magn_calibration = np.array([[1.013437,-0.04728858,-0.001861475],\n",
    "                   [-0.04728862,1.004832,0.008222118],\n",
    "                   [-0.001861605,0.008221965,0.9439077]])\n",
    "magn_offset = np.array([-150.4098,74.62431,630.9805])\n",
    "'''\n",
    "# Calibration\n",
    "df[['GyroX','GyroY','GyroZ']] = df[['GyroX','GyroY','GyroZ']] + gyro_offset.T\n",
    "df[['AccX','AccY','AccZ']] = np.dot(df[['AccX','AccY','AccZ']],acc_calibration.T) + acc_offset.T\n",
    "df[['MagnX','MagnY','MagnZ']] = np.dot(df[['MagnX','MagnY','MagnZ']],magn_calibration.T) + magn_offset.T\n",
    "\n",
    "#Add time column by taking time steps of 1/logFreq\n",
    "df['Time'] = np.arange(0, len(df['Log Freq']))/df['Log Freq'][0]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the data and choosing a time frame\n",
    "\n",
    "In order to analyze the data we need to pick a time frame. By inspection we have found that at the beginning of the data acquisition and towards the end the data is very noisy. So we have selected a time frame in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data and trim 10% on the lower end and 15% on the upper one (limits have been chosen according to the plots)\n",
    "\n",
    "lowlimit = df['Time'][int(0.1*len(df['Time']))]\n",
    "highlimit = df['Time'][int(0.85*len(df['Time']))]\n",
    "\n",
    "(fig, axs) = plt.subplots(3,1,figsize=(15,10), sharex=True)\n",
    "\n",
    "fig.suptitle(f'Data Visualization\\n Lower bound: {lowlimit} s. Upper bound: {highlimit} s.', fontsize = 15)\n",
    "\n",
    "axs[0].set_title('Accelerometer Readings')\n",
    "axs[0].plot(df['Time'], df['AccX'], color= 'teal',label='X')\n",
    "axs[0].plot(df['Time'], df['AccY'], color= 'lightsteelblue',label='Y')\n",
    "axs[0].plot(df['Time'], df['AccZ'], color= 'darkblue',label='Z')\n",
    "axs[0].axvline(x = lowlimit, color = 'red', alpha = 0.5)\n",
    "axs[0].axvline(x = highlimit, color = 'red', alpha = 0.5)\n",
    "axs[0].set_ylabel('Acceleration [mg]')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(df['Time'], df['GyroX'],color = 'indianred', label='X')\n",
    "axs[1].plot(df['Time'], df['GyroY'],color = 'red' , label='Y')\n",
    "axs[1].plot(df['Time'], df['GyroZ'],color = 'orangered' , label='Z')\n",
    "axs[1].set_title('Gyroscope Readings')\n",
    "axs[1].axvline(x = lowlimit, color = 'red', alpha = 0.5)\n",
    "axs[1].axvline(x = highlimit, color = 'red', alpha = 0.5)\n",
    "axs[1].set_ylabel('Angular Velocity [dps]')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(df['Time'], df['MagnX'],color = 'forestgreen' , label='X')\n",
    "axs[2].plot(df['Time'], df['MagnY'], color = 'lime', label='Y')\n",
    "axs[2].plot(df['Time'], df['MagnZ'], color = 'springgreen', label='Z')\n",
    "axs[2].set_title('Magnetometer Readings')\n",
    "axs[2].axvline(x = lowlimit, color = 'red', alpha = 0.5)\n",
    "axs[2].axvline(x = highlimit, color = 'red', alpha = 0.5)\n",
    "axs[2].set_xlabel('Time [s]')\n",
    "axs[2].set_ylabel('Magnetic Field [mG]')\n",
    "axs[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit time between thresholds\n",
    "\n",
    "We apply the threshold on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the misleading rows by trimming the data based on time\n",
    "df_timecut = df.loc[(df['Time'] >= lowlimit) & (df['Time'] <= highlimit)] #limit analysis to the specified time frame\n",
    "\n",
    "# Select the columns we need for the PCA, drop the unimportant ones\n",
    "df_time_frame = df_timecut.drop(['Log Mode', 'Log Freq', 'Timestamp', 'Time'], axis=1)\n",
    "display(df_time_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STATISTICAL QUANTITIES\n",
    "\n",
    "Find statistical quantities and use them to center our data in order to perform a PCA.\n",
    "\n",
    "We can now analyze the data. This is standard practice, we have decided to put the data into a dataframe, instead of just the standard \"describe\" method, because we will then standardize the data (by subtracting the mean and dividing by the standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.DataFrame({\n",
    "    'mean'  : df_time_frame.mean(),\n",
    "    'median': df_time_frame.median(),\n",
    "    'variance': df_time_frame.var(),\n",
    "    'standard deviation': df_time_frame.std(),\n",
    "    '25% percentile' : df_time_frame.quantile(q=0.25),\n",
    "    '75% percentile': df_time_frame.quantile(q=0.75)}).T\n",
    "\n",
    "print('Statistics of the data set')\n",
    "display(statistics)\n",
    "\n",
    "# Center the data\n",
    "centered_df = (df_time_frame - statistics.iloc[0])\n",
    "\n",
    "# Normalize the data\n",
    "standardized_df = centered_df/statistics.iloc[3]\n",
    "\n",
    "print('Standardized Data')\n",
    "display(standardized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "As we have a large amount of data features, we perform a Principal Component Analysis to\n",
    "get the lowest dimension dataset possible without missing important information. We start by studying the covariance\n",
    "matrix of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix of the data\n",
    "cov = np.cov(standardized_df.T) \n",
    "print('Covariance matrix is: \\n')\n",
    "print(cov)\n",
    "\n",
    "# Eigenvalues and eigenvectors of covariance matrix\n",
    "l, V = la.eig(cov)\n",
    "print('\\n')\n",
    "\n",
    "# Study most important variances in the dataset\n",
    "var = []\n",
    "for i in range(len(l)):\n",
    "    var.append(np.real_if_close(l[i]/l.sum()))\n",
    "for i in range(len(var)):\n",
    "    print('eigVec', V[:,i], 'with eigenvalue', np.round(np.real_if_close(l[i]),5), 'accounts for', np.round(var[i]*100,5), '% of the dataset variance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rotate the data in the new base, thus taking a linear combination of physical quantities.\n",
    "The goal here is to then reduce the number of principal components, by setting a threshold on the total amount of variance that we want to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order eigenvalues and eigenvectors according to variance importance\n",
    "idx = np.argsort(l)[::-1]  # sort the eigenvalues and eigenvectors\n",
    "l_sort = l[idx]\n",
    "V_sort = V[idx]\n",
    "tr = l.sum()\n",
    "\n",
    "# Rotate the dataset to the base of eigenvalues and eigenvectors\n",
    "rotated_data = np.dot(V.T, standardized_df.T) #bring the dataset in the eigenvector basis\n",
    "\n",
    "# Create a dataframe with the Principal Components of the dataset\n",
    "PC_names = []\n",
    "for i in range(1,len(l_sort)+1): PC_names.append(f\"PC{i}\")\n",
    "    \n",
    "dataset_rotated = pd.DataFrame(rotated_data.T, columns = PC_names)\n",
    "\n",
    "# Store the weights value of each PC in a dictionary\n",
    "weights = dict()\n",
    "i = 0\n",
    "for PC in PC_names:\n",
    "    weights[PC] = np.real_if_close(l_sort[i]/tr)\n",
    "    i += 1\n",
    "\n",
    "# Find out how many columns of the PCA we can drop while keeping 80% of the info\n",
    "S, j = 0, 0\n",
    "while S < 0.8:\n",
    "    S += weights[PC_names[j]]\n",
    "    j += 1\n",
    "info_PC = S\n",
    "n_PC = j\n",
    "print(\"Reduction of the number of PC and its accuracy:\")\n",
    "print(np.round(info_PC*100, 2), '%, number of PC = ', n_PC)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now drop the principal components that are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the results, we can lower the number of variables to n_PC\n",
    "dataset_rotated = dataset_rotated.drop(PC_names[n_PC:], axis = 1)\n",
    "display(dataset_rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FREQUENCY ANALYSIS\n",
    "\n",
    "Once we have our data prepared for working, we start by studying its frequency.\n",
    "This gives us an idea of the frequency domain of the Heart Beat. Note that this is only to give us an idea, the next sections will deal with cleaning up the noise through various techniques, i.e.: bandpass and wavelet decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freq = 200\n",
    "\n",
    "# FFT\n",
    "FFTs = {} # create a dictionary to store the FFT of each PC\n",
    "for i in dataset_rotated:\n",
    "    FFTs[i] = np.abs(np.fft.fft(dataset_rotated[i]))\n",
    "\n",
    "freq_FFT = np.fft.fftfreq(d = 1/sample_freq, n = len(dataset_rotated['PC1']))\n",
    "\n",
    "# Set frequency limits\n",
    "min_f = 30/60  #minimum for athletes is 30 beats per minute\n",
    "max_f = 120/60 #maximum (sustainable) is around 120 beats per minimum\n",
    "for i in FFTs:\n",
    "    FFTs[i][freq_FFT <= min_f] = 0\n",
    "    FFTs[i][freq_FFT >= max_f] = 0\n",
    "\n",
    "# As each PC gives the frequencies of the heartbeat, we can sum all of them (weighted) to get a better idea of the\n",
    "# heart rate of the subject\n",
    "total_FFT = np.zeros(len(FFTs['PC1']))\n",
    "for i in FFTs:\n",
    "    total_FFT += FFTs[i]\n",
    "\n",
    "# Plot the frequencies\n",
    "frequencies_fig = plt.figure(figsize=(15, 8))\n",
    "for i in FFTs:\n",
    "    plt.plot(freq_FFT*60, FFTs[i], label=i, alpha = 0.4)\n",
    "plt.plot(freq_FFT*60, total_FFT, label='Sum of FFT', color = 'firebrick', linewidth = 2)\n",
    "plt.xlim([min_f*60-10, max_f*60+10])\n",
    "plt.ylim([0, 5000])\n",
    "plt.xlabel('Beats per minute', fontsize = 16)\n",
    "plt.ylabel('Power', fontsize = 16)\n",
    "plt.title('Fast Fourier Transform of the data after PCA', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find the value of the highest peak\n",
    "idx_max = np.argmax(total_FFT)\n",
    "peak_freq = freq_FFT[idx_max]*60\n",
    "print(\"Highest peak is at:\", np.round(peak_freq,2), \"beats per minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER\n",
    "\n",
    "As we can see from observing the data in the frequency domain, the subject heart rate primarilly ranges from about 60 to 100 beats per minute.\n",
    "\n",
    "The goal now is to filter the data to get a clearer view of these heart rate values, and\n",
    "to analyze them with respect to time. To do so, we have implemented a simple filter pipeline consisting of a butterworth bandpass filter, followed by a wavelet filter. Each principal component signal is passed through the filter, and a filtered signal is returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Butterworth bandpass filter\n",
    "\n",
    "First, we define and use a butterworth bandpass. The scipy function takes a signal as input and returns a filtered signl occording to the parameters specified. Using the observations from the fourier analysis we set the frequencies of 50 and 90 as the critical frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a time column to the dataframe\n",
    "dataset_rotated['Time'] = np.arange(0, len(dataset_rotated['PC1']))/sample_freq\n",
    "\n",
    "# butterworth bandpass filter\n",
    "sos = signal.butter(4, [50,90], 'bp', fs = 400, output = 'sos')\n",
    "\n",
    "# Filter each PC signal and add to dictionary to visualize\n",
    "filtered_PCs = dict()\n",
    "for i in FFTs:\n",
    "    filtered_PCs[i] = signal.sosfilt(sos, dataset_rotated[i])\n",
    "\n",
    "# Plot and visualize results of the bandpass filter\n",
    "fig, axs = plt.subplots(n_PC,1, sharex=True, sharey=True, figsize=(10,10))\n",
    "fig.suptitle('Filtered Principal Components')\n",
    "ax = 0\n",
    "for i in filtered_PCs:\n",
    "    axs[ax].plot(dataset_rotated['Time'], dataset_rotated[i], alpha = 0.5, label='Original Signal')\n",
    "    axs[ax].plot(dataset_rotated['Time'], filtered_PCs[i], label='Filtered Signal')\n",
    "    axs[ax].set_title(f'Original and Bandpass Filtered Signals for {i}')\n",
    "    axs[ax].legend(loc=\"lower left\")\n",
    "    axs[ax].set_xlim([0,2])\n",
    "    axs[ax].set_ylim([-10,10])\n",
    "    ax += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wavelet transform\n",
    "\n",
    "This function takes as input the signal to be decomposed, the amount of levels for which it should be decomposed, the signal type to be used for the wavelet decomposition and a boolean variable to determine if the breakdown of the wavelet levels should be shown.\n",
    "\n",
    "The basic idea of the wavelet transform is to decompose a signal into a set of wavelets, sliding each wavelet scale across the signal, and determining how much of each respective scale is in the signal. In general, the lower levels of the wavelet decomposition correspond to higher frequencies and high levels correspond to low frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt # PyWavelets is open source wavelet transform software for Python\n",
    "import copy\n",
    "\n",
    "def wavelet_transform(signal, levels, waveletType, showWaveletScales):\n",
    "    # First, decompose signal\n",
    "    transformCoeffs = pywt.wavedec(signal, waveletType, level=levels)\n",
    "    \n",
    "    # Create a time array and a result array to visualize the layers\n",
    "    time = np.array(dataset_rotated['Time'])\n",
    "    result = np.zeros((levels+1, len(time)+1))\n",
    "    \n",
    "    # The following is purely for visualization purposes \n",
    "    for i in range(levels+1):\n",
    "        # Loop for every individual layer to get the coefficients of just that level\n",
    "        coeffs = copy.deepcopy(transformCoeffs)\n",
    "        \n",
    "        for j in range(levels+1):\n",
    "            # Loop over every other level and set to zero, we want to ignore all other levels\n",
    "            if(i!=j):\n",
    "                coeffs[j] = np.zeros_like(coeffs[j])\n",
    "        \n",
    "        #add the inverse wavelet of the level i to the corresponding result row\n",
    "        result[i] = pywt.waverec(coeffs, waveletType)\n",
    "        \n",
    "    # Option to visualize the wavelet scale decomposition\n",
    "    if showWaveletScales == True:\n",
    "        for level in range(levels+1):\n",
    "            # Plot each level\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.plot(time, result[level][:-1])\n",
    "            plt.title(f'Wavelet Transform of PC1 for level {level}')\n",
    "            plt.xlabel('Time')\n",
    "            plt.show()\n",
    "    \n",
    "    return transformCoeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wavelet Filter\n",
    "\n",
    "The following function is the actual wavelet filter, which receives the coefficients generated from the wavelet transform, the indices of the wavelet scales that we wish to filter out, and the signal type used in the decomposition. It returns the wavelet recomposition (inverse of the original decomposition) with the layers omitted in the indices parameter set to zero. \n",
    "\n",
    "From cardiography and wavelet literature, we chose to use 6 layers in the wavelet transformation and to filter out the first two scales and the last three scales, in order to remove both the high and low frequency noise in the signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletFilter(coeffs, indices, waveletType):\n",
    "    for level in range(len(coeffs)):\n",
    "        # Set every level in the indices parameter to zero\n",
    "        if level in indices:\n",
    "            coeffs[level] = np.zeros_like(coeffs[level])\n",
    "    # Return the reconstructed signal\n",
    "    return pywt.waverec(coeffs, waveletType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Pipeline\n",
    "\n",
    "Finally, we compiled the bandpass and wavelet functions into one filter pipeline that receives a signal and passes it through first the bandpass function and then the wavelet functions, and returns a filtered signal in the time domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSignal(sig):\n",
    "\n",
    "    # Passes a signal through a bandpass and wavelet filter and returns the filtered signal\n",
    "    bandpass = signal.sosfilt(sos, sig)\n",
    "    wavelet = wavelet_transform(np.array(bandpass), 5, 'sym4', False)\n",
    "    wavelet_filtered = waveletFilter(wavelet, [0,1,2,4,5], 'sym4')\n",
    "        # 0,1,2,5,6\n",
    "    #0, 1, 3, 4\n",
    "    return wavelet_filtered[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the Signals\n",
    "\n",
    "With the filter pipeline, we now create a dictionary and pass each principal component signal through the pipeline. We then assign the filtered signal to the corresponding principal component key and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filters to the data\n",
    "PCs = dict()\n",
    "for i in FFTs:\n",
    "    # Assign the filtered signal to each principal component\n",
    "    PCs[i] = filterSignal(dataset_rotated[i])\n",
    "\n",
    "# Plot each filtered signal\n",
    "(fig, axs) = plt.subplots(n_PC,1,figsize = (15,8), sharex=True, constrained_layout=True)\n",
    "fig.suptitle('Filtered signals over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.xlim(6, 16)\n",
    "ax = 0\n",
    "for i in PCs:\n",
    "    axs[ax].plot(np.array(dataset_rotated['Time']), (PCs[i]), label=i)\n",
    "    axs[ax].set_title(f'Filtered Signal for {i}')\n",
    "    axs[ax].set_ylabel('Amplitude')\n",
    "    ax += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Heart Beat Per Minute(BPM): extrapolate BPM, make an histogram of the result. Does it follow a partiular distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the peaks in the filtered data\n",
    "# scipy function find_peaks finds indexes of the peaks when provided:\n",
    "# height: minimum height of a measured peak\n",
    "# distance: minimum number of indexes between measured peaks\n",
    "\n",
    "# Function for, given the peak indexes, finding corresponding time and height\n",
    "def peak_points(indices, time, signl):\n",
    "    x_peaks = []\n",
    "    y_peaks = []\n",
    "    for i in indices:\n",
    "        x_peaks.append(time[i])\n",
    "        y_peaks.append(signl[i])\n",
    "    return np.array(x_peaks), np.array(y_peaks)\n",
    "\n",
    "PC_times = dict()\n",
    "\n",
    "for i in PCs:\n",
    "    lorenzetti = np.sum(np.abs(PCs[i])**(1/2))/(len(PCs[i]))\n",
    "    \n",
    "    # Find peaks of the signal and their times\n",
    "    peaks = signal.find_peaks((PCs[i]), height =lorenzetti, distance = 40)\n",
    "    time_PC, peaks_PC = peak_points(peaks[0], np.array(dataset_rotated['Time']), PCs[i])\n",
    "    \n",
    "    #Store the times of the peaks\n",
    "    PC_times[i] = time_PC\n",
    "    \n",
    "    #Plot peaks\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(np.array(dataset_rotated['Time']), np.abs(PCs[i]))\n",
    "    plt.plot(time_PC, (peaks_PC), '*')\n",
    "    plt.axhline(lorenzetti,color='red', label = f'Threshold = {np.round(lorenzetti,2)}')\n",
    "    plt.legend()\n",
    "    plt.title(f'Peaks for {i}')\n",
    "    plt.xlim([30,40])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should filter the points that are valuable and estimate the periodicity between peaks\n",
    "# we said highest possible frequency = 120bpm, therefore it is only reasonable\n",
    "# that there is a peak at least every 1/120*60 seconds = 0.5s\n",
    "\n",
    "# This function makes sure of the above\n",
    "def time_frame(t, a, b):\n",
    "    # break if it is out of bounds of time\n",
    "    if b > (len(t)-1): return 0, 0\n",
    "    measure = (t[b] - t[a])\n",
    "    if measure > 0.5:\n",
    "        return measure, b\n",
    "    else:\n",
    "        b += 1\n",
    "        return time_frame(t, a, b)\n",
    "\n",
    "    \n",
    "def heartbeat(times):\n",
    "    # Takes in times of the peaks, and returns the heartbeat values and \n",
    "    # times of the heartbeats\n",
    "    \n",
    "    periodicity = []\n",
    "    time_beat = []\n",
    "    b = 0\n",
    "    for i in range(len(times)):\n",
    "        if i == b:\n",
    "            p, b = time_frame(times, i, (i+1))\n",
    "            periodicity.append(p)\n",
    "            time_beat.append((times[b]+times[i])/2)\n",
    "    periodicity.pop()\n",
    "    time_beat.pop()\n",
    "    HB_values = 1/np.array(periodicity)*60\n",
    "\n",
    "    return HB_values, time_beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros(16)\n",
    "\n",
    "fig, axs = plt.subplots(n_PC, 2, figsize = (15, 15), gridspec_kw={'width_ratios': [3, 1]}, constrained_layout=True)\n",
    "plt.suptitle('Heart Rate measured with MuSe placed on the center of the sternum of a healthy subject', fontsize = 16)\n",
    "axs[n_PC-1,0].set_xlabel('Time (s)', fontsize = 13)\n",
    "axs[n_PC-1,1].set_xlabel('Heart Rate (BPM)', fontsize = 13)\n",
    "axs[0,0].set_title('Heart Rate as a function of time', fontsize = 14, pad = 12)\n",
    "axs[0,1].set_title('Heart Rate Distribution', fontsize = 14, pad = 12)\n",
    "ax = 0\n",
    "for i in PC_times:\n",
    "    HB_values, time_beat = heartbeat(PC_times[i])\n",
    "    #heartbeat as a function of time\n",
    "    axs[ax,0].plot(time_beat, HB_values, color = 'cyan')\n",
    "    axs[ax,0].plot(time_beat, HB_values, '*', label = i)\n",
    "    axs[ax,0].set_ylabel('Heart Rate (BPM)', fontsize = 15)\n",
    "    axs[ax,0].legend(loc = 'lower right')\n",
    "    axs[ax,0].set_ylim([0,120])\n",
    "    #histogram\n",
    "    n, bins, patches = axs[ax,1].hist(HB_values, bins=16, range=(40, 120))\n",
    "    axs[ax,1].set_ylim([0,30])\n",
    "    counts += n*weights[i]/info_PC\n",
    "    ax += 1\n",
    "\n",
    "plt.show()\n",
    "print(len(time_beat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "# Fit the histogram  to a gaussian\n",
    "def gauss(x, amp, mu, sigma):\n",
    "    #return height + amp*np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    return amp*np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "\n",
    "def bimodal(x, amp1, mu1, sigma1,amp2,mu2,sigma2):\n",
    "    return gauss(x,amp1,mu1,sigma1)+gauss(x,amp2, mu2,sigma2)\n",
    "\n",
    "\n",
    "def gamma(x,c,amp):\n",
    "    return amp*stats.gamma.pdf(x,c)\n",
    "\n",
    "X = np.arange(42.5,122.5,5)\n",
    "#popt, pcov = optimize.curve_fit(gauss, X, counts, p0 = (20, 69, 10))\n",
    "#popt, pcov = optimize.curve_fit(bimodal, X, counts, p0 = (0,69,5,0,88,5))\n",
    "popt, pcov = optimize.curve_fit(gamma, X, counts, p0 = (69,400))\n",
    "\n",
    "X_plot = np.arange(40,120,1)\n",
    "# Plot\n",
    "plt.figure(figsize = (10,8))\n",
    "#plt.plot(X_plot, bimodal(X_plot, *popt), color = 'red', label='Fit')\n",
    "#plt.plot(X_plot, gauss(X_plot, *popt), color = 'red', label='Fit')\n",
    "plt.plot(X_plot, gamma(X_plot, *popt), color = 'red', label='Fit')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.bar(X, counts, width=5, align='center')\n",
    "plt.title('Heart Rate Distribution', pad = 12)\n",
    "plt.xlabel('(BPM)')\n",
    "\n",
    "#'''Remove the # if you don't want the gamma with the standard deviations\n",
    "mean, var, skew, kurt = stats.gamma.stats(popt[0], moments = 'mvsk')\n",
    "plt.axvline(x = mean + np.sqrt(var), linestyle = 'dotted',linewidth = 2,color = 'red')\n",
    "plt.axvline(x = mean - np.sqrt(var),linestyle = 'dotted',linewidth = 2, color = 'red')\n",
    "plt.axvline(x = mean + 2*np.sqrt(var), linestyle = 'dotted', linewidth = 2, color = 'red', alpha = 0.7)\n",
    "plt.axvline(x = mean - 2*np.sqrt(var),linestyle = 'dotted',linewidth = 2, color = 'red', alpha = 0.7)\n",
    "plt.axvline(x = mean + 3*np.sqrt(var), linestyle = 'dotted',linewidth = 2,color = 'red', alpha = 0.5)\n",
    "plt.axvline(x = mean - 3*np.sqrt(var),linestyle = 'dotted',linewidth = 2, color = 'red', alpha = 0.5)\n",
    "plt.annotate(text='', xy=(mean + np.sqrt(var),17.5), xytext=(mean - np.sqrt(var),17.5), arrowprops=dict(arrowstyle='<->'))\n",
    "plt.annotate(text='', xy=(mean + 2*np.sqrt(var),16), xytext=(mean - 2*np.sqrt(var),16), arrowprops=dict(arrowstyle='<->'))\n",
    "plt.annotate(text='', xy=(mean + 3*np.sqrt(var),14.5), xytext=(mean - 3*np.sqrt(var),14.5), arrowprops=dict(arrowstyle='<->'))\n",
    "t = plt.text(mean, 17.5, \"1σ\", ha=\"center\", va=\"center\", rotation=0, size=15, bbox=dict(boxstyle=\"square, pad=0.3\", fc=\"white\", ec=\"b\", lw=1))\n",
    "t1 = plt.text(mean, 16, \"2σ\", ha=\"center\", va=\"center\", rotation=0, size=15, bbox=dict(boxstyle=\"square, pad=0.3\", fc=\"white\", ec=\"b\", lw=1))\n",
    "t2 = plt.text(mean, 14.5, \"3σ\", ha=\"center\", va=\"center\", rotation=0, size=15, bbox=dict(boxstyle=\"square, pad=0.3\", fc=\"white\", ec=\"b\", lw=1))\n",
    "#'''\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty cell, do not delete, just edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Heart Rate Variability(HRV): extrapolate HRV, explain why this parameter is important, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty cell, do not delete, just edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty cell, do not delete, just edit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
